{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "#### Overview\n",
    "The data has been split into two groups:\n",
    "\n",
    "1. **Training set (`train.csv`)**: This set should be used to build your machine learning models. The training set provides the outcome (also known as the \"ground truth\") for each passenger. Your model will be based on \"features\" like passengers' gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "2. **Test set (`test.csv`)**: This set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include `gender_submission.csv`, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "\n",
    "#### Data Dictionary\n",
    "| Variable  | Definition                       | Key                                       |\n",
    "|-----------|----------------------------------|-------------------------------------------|\n",
    "| survival  | Survival                         | 0 = No, 1 = Yes                           |\n",
    "| pclass    | Ticket class                     | 1 = 1st, 2 = 2nd, 3 = 3rd                 |\n",
    "| sex       | Sex                              |                                           |\n",
    "| Age       | Age in years                     |                                           |\n",
    "| sibsp     | # of siblings / spouses aboard   |                                           |\n",
    "| parch     | # of parents / children aboard   |                                           |\n",
    "| ticket    | Ticket number                    |                                           |\n",
    "| fare      | Passenger fare                   |                                           |\n",
    "| cabin     | Cabin number                     |                                           |\n",
    "| embarked  | Port of Embarkation              | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "##### Variable Notes\n",
    "- **pclass**: A proxy for socio-economic status (SES)\n",
    "  - 1st = Upper\n",
    "  - 2nd = Middle\n",
    "  - 3rd = Lower\n",
    "- **age**: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5\n",
    "- **sibsp**: The dataset defines family relations in this way:\n",
    "  - Sibling = brother, sister, stepbrother, stepsister\n",
    "  - Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "- **parch**: The dataset defines family relations in this way:\n",
    "  - Parent = mother, father\n",
    "  - Child = daughter, son, stepdaughter, stepson\n",
    "  - Some children traveled only with a nanny, therefore parch=0 for them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a crucial step before diving into machine learning or statistical modeling. It helps us understand the nature of the data, identify patterns, spot anomalies, and form hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataframe\n",
    "train_df = pd.read_csv('data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first few rows\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe info\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the following columns from the dataset:\n",
    "- `PassengerId`: Passenger ID, not useful for our analysis\n",
    "- `Name`: Passenger name, not useful for our analysis\n",
    "- `Ticket`: Ticket number, not useful for our analysis\n",
    "- `Cabin`: Cabin number, it has too many missing values\n",
    "- `Embarked`: Port of Embarkation, not useful for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate the features from the target\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "target = 'Survived'\n",
    "\n",
    "# Let's select only the features we want and the target for data analysis\n",
    "train_df = train_df[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic Stadistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary stadistics for numerical columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pclass**: Ranges from 1 to 3. The mean is around 2.31, indicating that there are more passengers in the lower classes.\n",
    "- **Age**: Ranges from 0.42 to 80 years. The mean age is approximately 29.7 years. Note that the count is less than the total number of passengers, indicating missing values.\n",
    "- **SibSp**: Ranges from 0 to 8. The mean is approximately 0.52, indicating that most passengers did not have siblings or spouses aboard.\n",
    "- **Parch**: Ranges from 0 to 6. The mean is around 0.38, suggesting that most passengers were not traveling with parents or children.\n",
    "- **Fare**: Ranges from 0 to 512.33. The mean fare is approximately 32.2. The standard deviation is high, indicating a wide range of fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 177 missing values in the `Age` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Distribution numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot histograms for the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "colors = {\n",
    "    'Pclass': ['blue', 'green', 'red'],\n",
    "    'Sex': ['purple', 'orange'],\n",
    "    'SibSp': ['gray'] * train_df['SibSp'].nunique(),\n",
    "    'Parch': ['cyan'] * train_df['Parch'].nunique()\n",
    "}\n",
    "\n",
    "# Pclass distribution\n",
    "train_df['Pclass'].value_counts().sort_index().plot(kind='bar', ax=ax[0, 0], title='Pclass distribution', color=colors['Pclass'], grid=True)\n",
    "ax[0, 0].grid(axis='x')\n",
    "# Sex distribution\n",
    "train_df['Sex'].value_counts().plot(kind='bar', ax=ax[0][1], title=\"Sex distribution\", color=colors['Sex'], grid=True)\n",
    "ax[0, 1].grid(axis='x')\n",
    "# Age distribution\n",
    "train_df['Age'].plot(kind='hist', ax=ax[0][2], bins=30, title=\"Age distribution\", grid=True)\n",
    "# SibSp distribution\n",
    "train_df['SibSp'].value_counts().sort_index().plot(kind='bar', ax=ax[1][0], title=\"SibSp distribution\", color=colors['SibSp'], grid=True)\n",
    "ax[1, 0].grid(axis='x')\n",
    "# Parch distribution\n",
    "train_df['Parch'].value_counts().sort_index().plot(kind='bar', ax=ax[1][1], title=\"Parch distribution\", color=colors['Parch'], grid=True)\n",
    "ax[1, 1].grid(axis='x')\n",
    "# Fare distribution\n",
    "train_df['Fare'].plot(kind='hist', ax=ax[1][2], bins=30, title=\"Fare distribution\", grid=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pclass Distribution**: Most passengers are in the 3rd class, followed by 1st and 2nd class.\n",
    "- **Sex Distribution**: There are more male passengers than female passengers.\n",
    "- **Age Distribution**: The majority of passengers are in the age range of 20-40 years. The distribution is slightly right-skewed with a smaller number of elderly passengers.\n",
    "- **SibSp Distribution**: Most passengers traveled without siblings or spouses, with fewer passengers having one or more siblings/spouses.\n",
    "- **Parch Distribution**: Similarly, most passengers traveled without parents or children aboard.\n",
    "- **Fare Distribution**: Most fares are on the lower side, with a few passengers paying significantly higher fares. This distribution is right-skewed, indicating potential outliers on the higher side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create box plots for the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 8))\n",
    "\n",
    "train_df.boxplot(column=\"Age\", ax=ax[0], boxprops=dict(linestyle='-', linewidth=2, color='red'))\n",
    "train_df.boxplot(column=\"SibSp\", ax=ax[1], boxprops=dict(linestyle='-', linewidth=2, color='red'))\n",
    "train_df.boxplot(column=\"Parch\", ax=ax[2], boxprops=dict(linestyle='-', linewidth=2, color='red'))\n",
    "train_df.boxplot(column=\"Fare\", ax=ax[3], boxprops=dict(linestyle='-', linewidth=2, color='red'))\n",
    "\n",
    "ax[0].set_title(\"Boxplot of Age\")\n",
    "ax[1].set_title(\"Boxplot of SibSp\")\n",
    "ax[2].set_title(\"Botplot of Parch\")\n",
    "ax[3].set_title(\"Boxplot of Fare\")\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].grid(axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boxplot of Age**: The majority of passengers fall within the 20-40 age range, with some outliers on the higher side.\n",
    "- **Boxplot of SibSp**: Most passengers traveled without siblings or spouses. The higher values (above 2) can be seen as outliers.\n",
    "- **Boxplot of Parch**: Most passengers traveled without parents or children. The higher values (above 2) can be considered outliers.\n",
    "- **Boxplot of Fare**: While most fares are on the lower side, there are several outliers on the higher side, indicating passengers who paid significantly more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Distribution target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# First at all, we plot target distribution\n",
    "sns.countplot(x='Survived', data=train_df)\n",
    "plt.title(\"Survived distribution\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(20, 12))\n",
    "\n",
    "# Convert the Survived column to a string object\n",
    "train_df['Survived'] = train_df['Survived'].astype(str)\n",
    "\n",
    "# Now, we plot the distribution of the target by Pclass\n",
    "sns.countplot(x='Pclass', hue='Survived', data=train_df, ax=axs[0][0]);\n",
    "axs[0][0].set_title('Survived distribution by Pclass')\n",
    "axs[0][0].grid(axis='y')\n",
    "\n",
    "# Plot the distribution of the target by Sex\n",
    "sns.countplot(x='Sex', hue='Survived', data=train_df, ax=axs[0][1])\n",
    "axs[0][1].set_title('Survived distribution by Sex')\n",
    "axs[0][1].grid(axis='y')\n",
    "\n",
    "# Plot the distribution of the target by Age\n",
    "sns.boxplot(x='Survived', y='Age', data=train_df, ax=axs[0][2])\n",
    "axs[0][2].set_title('Survived distribution by Age')\n",
    "\n",
    "# Plot the distribution of the target by SibSp\n",
    "sns.countplot(x='SibSp', hue='Survived', data=train_df, ax=axs[1][0])\n",
    "axs[1][0].set_title('Survived distribution by SibSp')\n",
    "axs[1][0].grid(axis='y')\n",
    "\n",
    "# Plot the distribution of the target by Parch\n",
    "sns.countplot(x='Parch', hue='Survived', data=train_df, ax=axs[1][1])\n",
    "axs[1][1].set_title('Survived distribution by Parch')\n",
    "axs[1][1].grid(axis='y')\n",
    "\n",
    "# Plot the distribution of the target by Fare\n",
    "sns.boxplot(x='Survived', y='Fare', data=train_df, ax=axs[1][2])\n",
    "axs[1][2].set_title('Survived distribution by Fare')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualizations, we can observe several patterns, such as:\n",
    "\n",
    "- A higher proportion of females survived compared to males.\n",
    "- Higher passenger classes (i.e., 1st class) seem to have a higher survival rate.\n",
    "- Age, fare, and the number of siblings/spouses (SibSp) and parents/children (Parch) aboard also show variations in the survival rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Representa el pairplot\n",
    "sns.pairplot(train_df, hue = 'Survived', vars = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"], palette = 'husl', diag_kind = 'kde', kind = 'scatter', height=3);\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Sex' column to numerical values for correlation calculation\n",
    "corr_df = train_df.copy()\n",
    "corr_df['Sex'] = corr_df['Sex'].map({'male': 0, 'female': 1})\n",
    "corr_df['Survived'] = corr_df['Survived'].astype(int)\n",
    "\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "print(corr_matrix['Survived'].sort_values(ascending=False))\n",
    "print(corr_matrix)\n",
    "\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no strong correlation between the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the missing values in the `Age` column with the mean age based on `Sex` and `Pclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Sex and Pclass and calculate the mean of Age\n",
    "mean_ages = train_df.groupby(['Sex', 'Pclass'])['Age'].mean()\n",
    "\n",
    "# Define function to fill missing values for age\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return mean_ages[row['Sex'], row['Pclass']]\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "train_df['Age'] = train_df.apply(fill_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are missing values\n",
    "train_df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/titanic/train_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv('data/titanic/train_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First at all, le'ts encode the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_encoded = pd.get_dummies(titanic_data, columns=[\"Sex\", \"Pclass\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_data_encoded.drop('Survived', axis=1)\n",
    "y = titanic_data_encoded['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Let's display the shapes of the train and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply different algoritms to predict the survival of passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(gs, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Performs a grid search using the provided GridSearchCV o RandomizedSearchCV object on the provided training data.\n",
    "    \n",
    "    Parameters:\n",
    "    gs (GridSearchCV o RandomizedSearchCV): The search object.\n",
    "    X_train (array-like): The feature matrix for the training data.\n",
    "    y_train (array-like): The labels for the training data.\n",
    "    \n",
    "    Returns:\n",
    "    The best estimator found during the grid search.\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Fit the GridSearchCV object to the training data\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best hyperparameters found during the grid search\n",
    "    print(\"Best parameters:\", gs.best_params_)\n",
    "    \n",
    "    # Print the highest mean cross-validated score achieved by the best estimator\n",
    "    print(\"Best score:\", gs.best_score_)\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print(\"Best estimator:\", gs.best_estimator_)\n",
    "    \n",
    "    warnings.resetwarnings()\n",
    "\n",
    "    # Return the best estimator\n",
    "    return gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_report(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generates a classification report for the provided model on the provided test data.\n",
    "    \n",
    "    Parameters:\n",
    "    model (sklearn estimator): The model to generate the classification report for.\n",
    "    X_test (array-like): The feature matrix for the test data.\n",
    "    y_test (array-like): The labels for the test data.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the accuracy score of the model\n",
    "    print(\"Accuracy score:\", accuracy_score(y_test, y_pred), end=\"\\n\\n\")\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g')\n",
    "\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_AUC_curves(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Plots the ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve)\n",
    "    scores for the given model, using both training and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    model (object): The classifier model to evaluate.\n",
    "    X_train (array-like): The feature matrix for the training data.\n",
    "    y_train (array-like): The labels for the training data.\n",
    "    X_test (array-like): The feature matrix for the testing data.\n",
    "    y_test (array-like): The labels for the testing data.\n",
    "    model_name (str): A name for the model, used in the plot title.\n",
    "    \n",
    "    Returns:\n",
    "    None. Displays the plot using plt.show().\n",
    "    \"\"\"\n",
    "    # Get the probabilities of the positive class for training and testing data\n",
    "    train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate the ROC curve for training data\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, train_probs)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    \n",
    "    # Calculate the ROC curve for test data\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, test_probs)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    # Set up the figure and axis for the plot, with a specified size\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Plot the ROC curve for the training data\n",
    "    plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Train ROC curve (area = {roc_auc_train:.2f})')\n",
    "    \n",
    "    # Plot the ROC curve for the testing data\n",
    "    plt.plot(fpr_test, tpr_test, color='red', lw=2, label=f'Test ROC curve (area = {roc_auc_test:.2f})')\n",
    "    \n",
    "    # Plot a gray dashed line representing random guessing\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    \n",
    "    # Set the limits of the x and y axes\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    # Label the axes and the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve for {model_name}')\n",
    "    \n",
    "    # Add a legend to the plot\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Add a grid to the plot\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform a grid search to find the best hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    param_grid_lr, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lr = perform_search(grid_search, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(lr, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the ROC curve to determine the performance of the model and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(lr, X_train_scaled, y_train, X_test_scaled, y_test, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ROC curve for the training data (in blue) has an area under the curve (AUC) of approximately 0.87.\n",
    "\n",
    "- The ROC curve for the test data (in red) has an AUC of approximately 0.85.\n",
    "\n",
    "- Both curves are relatively close to each other, which suggests that the model isn't overfitting significantly. If there was a large gap between the training and test ROC curves, it would be more indicative of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a Grid Search to find the best hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn = {\n",
    "    'n_neighbors': range(1, 30),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "knn = perform_search(grid_search_knn, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(knn, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(knn, X_train_scaled, y_train, X_test_scaled, y_test, 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some overfitting, more than the Logistic Regression model. And the accuracy score is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a Grid Search to find the best hyperparameters for the model Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(1, 21),\n",
    "    'min_samples_split': range(2, 21),\n",
    "    'min_samples_leaf': range(1, 21),\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid_dt, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "dt = perform_search(grid_search_dt, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(dt, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(dt, X_train_scaled, y_train, X_test_scaled, y_test, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result with this model is not improving the accuracy score. And also, there is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],    \n",
    "    'gamma': [0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    param_grid_svm, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "svm = perform_search(random_search_svm, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(svm, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(svm, X_train_scaled, y_train, X_test_scaled, y_test, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the ROC curve that there is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [130, 150, 170],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2],\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid_rf, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf = perform_search(grid_search_rf, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(rf, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(rf, X_train_scaled, y_train, X_test_scaled, y_test, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Machines (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'max_depth': [2, 3, 4, 5,],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xgb, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    verbose=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb = perform_search(grid_search_xgb, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report(xgb, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_AUC_curves(xgb, X_train_scaled, y_train, X_test_scaled, y_test, 'XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks / Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
